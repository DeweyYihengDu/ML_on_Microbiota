{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1\n",
    "\n",
    "Use the other bactria orders to predict the Top5 bacteria abundance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DuYih\\AppData\\Local\\Temp\\ipykernel_5212\\1084890260.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[col] = le.fit_transform(train_df[col])\n",
      "C:\\Users\\DuYih\\AppData\\Local\\Temp\\ipykernel_5212\\1084890260.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[col] = le.transform(test_df[col])\n",
      "C:\\Users\\DuYih\\AppData\\Local\\Temp\\ipykernel_5212\\1084890260.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[col] = le.fit_transform(train_df[col])\n",
      "C:\\Users\\DuYih\\AppData\\Local\\Temp\\ipykernel_5212\\1084890260.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[col] = le.transform(test_df[col])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lars, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneOut\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "# Load data\n",
    "file_path = '../../matrix/otu_merged_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Function to map drug concentration\n",
    "def map_drug_concentration(value):\n",
    "    if value == 0:\n",
    "        return \"none\"\n",
    "    elif value == 5 or value == 10:\n",
    "        return \"low\"\n",
    "    elif value == 50 or value == 100:\n",
    "        return \"high\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "# Create 'Drug Set' column\n",
    "drug_columns = ['amoxicillin', 'oxytetracycline_dihydrate', 'sulfadiazine', 'trimethoprim', 'tylosin_tartrate', 'ciprofloxacin']\n",
    "df['Drug Set'] = df[drug_columns].apply(lambda row: '_'.join([map_drug_concentration(x) for x in row]), axis=1)\n",
    "\n",
    "# Filter data to only include specific bacterial families\n",
    "bacterial_families = [\"o__Bacillales;\", \"o__Lactobacillales;\", \"o__Enterobacteriales;\", \"o__Burkholderiales;\",\n",
    "                      \"o__Actinomycetales;\", \"o__Aeromonadales;\", \"o__Pseudomonadales;\"]\n",
    "df_filtered = df[['SampleID', 'Group', 'Isolation_source', 'Drug Set'] + bacterial_families]\n",
    "\n",
    "# Prepare training and test sets\n",
    "train_df = df_filtered[df_filtered['Group'].isin(['G1', 'G2', 'G3'])]\n",
    "test_df = df_filtered[df_filtered['Group'] == 'G4']\n",
    "\n",
    "# Label encode the categorical features\n",
    "label_encoders = {}\n",
    "for col in ['Isolation_source', 'Drug Set']:\n",
    "    le = LabelEncoder()\n",
    "    train_df[col] = le.fit_transform(train_df[col])\n",
    "    test_df[col] = le.transform(test_df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Define features and target variables\n",
    "X_train = train_df[['Isolation_source', 'Drug Set']]\n",
    "y_train = train_df[bacterial_families]\n",
    "X_test = test_df[['Isolation_source', 'Drug Set']]\n",
    "y_test = test_df[bacterial_families]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Least Angle Regression': Lars(),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'Lasso': Lasso(),\n",
    "    'Elastic-Net': ElasticNet()\n",
    "}\n",
    "\n",
    "# Initialize evaluation metrics dictionary\n",
    "evaluation_metrics = {}\n",
    "\n",
    "# Train and evaluate models\n",
    "for family in bacterial_families:\n",
    "    evaluation_metrics[family] = {}\n",
    "    for model_name, model in models.items():\n",
    "        # Use Leave-One-Out Cross-Validation\n",
    "        loo = LeaveOneOut()\n",
    "        rmse_scores = np.sqrt(-cross_val_score(model, X_train_scaled, y_train[family], cv=loo, scoring='neg_mean_squared_error'))\n",
    "        \n",
    "        rmse_mean = np.mean(rmse_scores)\n",
    "        rmse_std_error = stats.sem(rmse_scores)\n",
    "        \n",
    "        # Fit on the entire training set and evaluate on the test set\n",
    "        model.fit(X_train_scaled, y_train[family])\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test[family], y_pred))\n",
    "        \n",
    "        # Store evaluation metrics\n",
    "        evaluation_metrics[family][model_name] = {\n",
    "            'Train RMSE': rmse_mean,\n",
    "            'Train Standard Error': rmse_std_error,\n",
    "            'Test RMSE': test_rmse\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 425\n",
      "Total testing samples: 202\n",
      "Total samples for validation (Leave-One-Out): 425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DuYih\\AppData\\Local\\Temp\\ipykernel_11508\\3607495147.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[col] = le.fit_transform(train_df[col])\n",
      "C:\\Users\\DuYih\\AppData\\Local\\Temp\\ipykernel_11508\\3607495147.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[col] = le.transform(test_df[col])\n",
      "C:\\Users\\DuYih\\AppData\\Local\\Temp\\ipykernel_11508\\3607495147.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[col] = le.fit_transform(train_df[col])\n",
      "C:\\Users\\DuYih\\AppData\\Local\\Temp\\ipykernel_11508\\3607495147.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[col] = le.transform(test_df[col])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lars, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneOut\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Load data\n",
    "file_path = '../../matrix/otu_merged_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Function to map drug concentration\n",
    "def map_drug_concentration(value):\n",
    "    if value == 0:\n",
    "        return \"none\"\n",
    "    elif value == 5 or value == 10:\n",
    "        return \"low\"\n",
    "    elif value == 50 or value == 100:\n",
    "        return \"high\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "# Create 'Drug Set' column\n",
    "drug_columns = ['amoxicillin', 'oxytetracycline_dihydrate', 'sulfadiazine', 'trimethoprim', 'tylosin_tartrate', 'ciprofloxacin']\n",
    "df['Drug Set'] = df[drug_columns].apply(lambda row: '_'.join([map_drug_concentration(x) for x in row]), axis=1)\n",
    "\n",
    "# Filter data to only include specific bacterial families\n",
    "bacterial_families = [\"o__Bacillales;\", \"o__Lactobacillales;\", \"o__Enterobacteriales;\", \"o__Burkholderiales;\",\n",
    "                      \"o__Actinomycetales;\", \"o__Aeromonadales;\", \"o__Pseudomonadales;\"]\n",
    "df_filtered = df[['SampleID', 'Group', 'Isolation_source', 'Drug Set'] + bacterial_families]\n",
    "\n",
    "# Prepare training and test sets\n",
    "train_df = df_filtered[df_filtered['Group'].isin(['G1', 'G2', 'G3'])]\n",
    "test_df = df_filtered[df_filtered['Group'] == 'G4']\n",
    "\n",
    "# Count the number of samples used for training and testing\n",
    "total_train_samples = train_df.shape[0]\n",
    "total_test_samples = test_df.shape[0]\n",
    "print(f\"Total training samples: {total_train_samples}\")\n",
    "print(f\"Total testing samples: {total_test_samples}\")\n",
    "\n",
    "# Label encode the categorical features\n",
    "label_encoders = {}\n",
    "for col in ['Isolation_source', 'Drug Set']:\n",
    "    le = LabelEncoder()\n",
    "    train_df[col] = le.fit_transform(train_df[col])\n",
    "    test_df[col] = le.transform(test_df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Define features and target variables\n",
    "X_train = train_df[['Isolation_source', 'Drug Set']]\n",
    "y_train = train_df[bacterial_families]\n",
    "X_test = test_df[['Isolation_source', 'Drug Set']]\n",
    "y_test = test_df[bacterial_families]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Initialize models\n",
    "# models = {\n",
    "#     'Least Angle Regression': Lars(),\n",
    "#     'Random Forest': RandomForestRegressor(),\n",
    "#     'Lasso': Lasso(),\n",
    "#     'Elastic-Net': ElasticNet()\n",
    "# }\n",
    "\n",
    "# # Initialize evaluation metrics dictionary\n",
    "# evaluation_metrics = {}\n",
    "\n",
    "# # Train and evaluate models\n",
    "# for family in bacterial_families:\n",
    "#     evaluation_metrics[family] = {}\n",
    "#     for model_name, model in models.items():\n",
    "#         # Use Leave-One-Out Cross-Validation\n",
    "#         loo = LeaveOneOut()\n",
    "#         rmse_scores = np.sqrt(-cross_val_score(model, X_train_scaled, y_train[family], cv=loo, scoring='neg_mean_squared_error'))\n",
    "        \n",
    "#         rmse_mean = np.mean(rmse_scores)\n",
    "#         rmse_std_error = stats.sem(rmse_scores)\n",
    "        \n",
    "#         # Fit on the entire training set and evaluate on the test set\n",
    "#         model.fit(X_train_scaled, y_train[family])\n",
    "#         y_pred = model.predict(X_test_scaled)\n",
    "#         test_rmse = np.sqrt(mean_squared_error(y_test[family], y_pred))\n",
    "        \n",
    "#         # Store evaluation metrics\n",
    "#         evaluation_metrics[family][model_name] = {\n",
    "#             'Train RMSE': rmse_mean,\n",
    "#             'Train Standard Error': rmse_std_error,\n",
    "#             'Test RMSE': test_rmse\n",
    "#         }\n",
    "\n",
    "# Output the total number of samples involved in training and validation\n",
    "print(f\"Total samples for validation (Leave-One-Out): {X_train.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'o__Bacillales;': {'Least Angle Regression': {'Train RMSE': np.float64(0.0038349319622816213),\n",
       "   'Train Standard Error': np.float64(0.0002299360030446969),\n",
       "   'Test RMSE': np.float64(0.005937985356894864)},\n",
       "  'Random Forest': {'Train RMSE': np.float64(0.003927711597789238),\n",
       "   'Train Standard Error': np.float64(0.0002324900662284059),\n",
       "   'Test RMSE': np.float64(0.005965682650459183)},\n",
       "  'Lasso': {'Train RMSE': np.float64(0.004374269283673696),\n",
       "   'Train Standard Error': np.float64(0.00024933531183090387),\n",
       "   'Test RMSE': np.float64(0.006586968278482531)},\n",
       "  'Elastic-Net': {'Train RMSE': np.float64(0.004374269283673696),\n",
       "   'Train Standard Error': np.float64(0.00024933531183090387),\n",
       "   'Test RMSE': np.float64(0.006586968278482531)}},\n",
       " 'o__Lactobacillales;': {'Least Angle Regression': {'Train RMSE': np.float64(4.16189085578734e-05),\n",
       "   'Train Standard Error': np.float64(5.361665987830978e-06),\n",
       "   'Test RMSE': np.float64(0.00013801865616042342)},\n",
       "  'Random Forest': {'Train RMSE': np.float64(4.179142196981802e-05),\n",
       "   'Train Standard Error': np.float64(5.473464693917461e-06),\n",
       "   'Test RMSE': np.float64(0.00013893041840316533)},\n",
       "  'Lasso': {'Train RMSE': np.float64(4.110265724750278e-05),\n",
       "   'Train Standard Error': np.float64(5.348479765039018e-06),\n",
       "   'Test RMSE': np.float64(0.00013725344186574977)},\n",
       "  'Elastic-Net': {'Train RMSE': np.float64(4.110265724750278e-05),\n",
       "   'Train Standard Error': np.float64(5.348479765039018e-06),\n",
       "   'Test RMSE': np.float64(0.00013725344186574977)}},\n",
       " 'o__Enterobacteriales;': {'Least Angle Regression': {'Train RMSE': np.float64(3.533202476688247e-05),\n",
       "   'Train Standard Error': np.float64(2.7103801479073303e-06),\n",
       "   'Test RMSE': np.float64(3.9490798832112204e-05)},\n",
       "  'Random Forest': {'Train RMSE': np.float64(3.578226054878789e-05),\n",
       "   'Train Standard Error': np.float64(2.720677926163128e-06),\n",
       "   'Test RMSE': np.float64(4.019232520894061e-05)},\n",
       "  'Lasso': {'Train RMSE': np.float64(3.536594037735849e-05),\n",
       "   'Train Standard Error': np.float64(2.743491310530007e-06),\n",
       "   'Test RMSE': np.float64(3.91357924840845e-05)},\n",
       "  'Elastic-Net': {'Train RMSE': np.float64(3.536594037735849e-05),\n",
       "   'Train Standard Error': np.float64(2.743491310530007e-06),\n",
       "   'Test RMSE': np.float64(3.91357924840845e-05)}},\n",
       " 'o__Burkholderiales;': {'Least Angle Regression': {'Train RMSE': np.float64(0.0036612595846989755),\n",
       "   'Train Standard Error': np.float64(0.00017838101487524605),\n",
       "   'Test RMSE': np.float64(0.005094430545853237)},\n",
       "  'Random Forest': {'Train RMSE': np.float64(0.003706405795713561),\n",
       "   'Train Standard Error': np.float64(0.00017110394137226755),\n",
       "   'Test RMSE': np.float64(0.005022928105786701)},\n",
       "  'Lasso': {'Train RMSE': np.float64(0.0068744942136071035),\n",
       "   'Train Standard Error': np.float64(0.0001691582170717415),\n",
       "   'Test RMSE': np.float64(0.007869661574750053)},\n",
       "  'Elastic-Net': {'Train RMSE': np.float64(0.0068744942136071035),\n",
       "   'Train Standard Error': np.float64(0.0001691582170717415),\n",
       "   'Test RMSE': np.float64(0.007869661574750053)}},\n",
       " 'o__Actinomycetales;': {'Least Angle Regression': {'Train RMSE': np.float64(0.005848984849956934),\n",
       "   'Train Standard Error': np.float64(0.00029035363008102894),\n",
       "   'Test RMSE': np.float64(0.0076303876228048205)},\n",
       "  'Random Forest': {'Train RMSE': np.float64(0.005986216605761998),\n",
       "   'Train Standard Error': np.float64(0.0002947296674940188),\n",
       "   'Test RMSE': np.float64(0.00758027512009541)},\n",
       "  'Lasso': {'Train RMSE': np.float64(0.010222283052075473),\n",
       "   'Train Standard Error': np.float64(0.0003582006330846086),\n",
       "   'Test RMSE': np.float64(0.01199533673866266)},\n",
       "  'Elastic-Net': {'Train RMSE': np.float64(0.010222283052075473),\n",
       "   'Train Standard Error': np.float64(0.0003582006330846086),\n",
       "   'Test RMSE': np.float64(0.01199533673866266)}},\n",
       " 'o__Aeromonadales;': {'Least Angle Regression': {'Train RMSE': np.float64(6.016959537484578e-05),\n",
       "   'Train Standard Error': np.float64(1.838171410413929e-05),\n",
       "   'Test RMSE': np.float64(8.303280882492503e-05)},\n",
       "  'Random Forest': {'Train RMSE': np.float64(6.353082699728909e-05),\n",
       "   'Train Standard Error': np.float64(1.870658074440764e-05),\n",
       "   'Test RMSE': np.float64(9.685837206407939e-05)},\n",
       "  'Lasso': {'Train RMSE': np.float64(5.77436873473918e-05),\n",
       "   'Train Standard Error': np.float64(1.8361323755277462e-05),\n",
       "   'Test RMSE': np.float64(8.324060369309083e-05)},\n",
       "  'Elastic-Net': {'Train RMSE': np.float64(5.77436873473918e-05),\n",
       "   'Train Standard Error': np.float64(1.8361323755277462e-05),\n",
       "   'Test RMSE': np.float64(8.324060369309083e-05)}},\n",
       " 'o__Pseudomonadales;': {'Least Angle Regression': {'Train RMSE': np.float64(0.0013301534013766883),\n",
       "   'Train Standard Error': np.float64(6.387719258367069e-05),\n",
       "   'Test RMSE': np.float64(0.0016378296970712273)},\n",
       "  'Random Forest': {'Train RMSE': np.float64(0.0013612709117925594),\n",
       "   'Train Standard Error': np.float64(6.445031771865508e-05),\n",
       "   'Test RMSE': np.float64(0.0016188248959849675)},\n",
       "  'Lasso': {'Train RMSE': np.float64(0.0013425880037957823),\n",
       "   'Train Standard Error': np.float64(6.344871948854839e-05),\n",
       "   'Test RMSE': np.float64(0.0016017600141737228)},\n",
       "  'Elastic-Net': {'Train RMSE': np.float64(0.0013425880037957823),\n",
       "   'Train Standard Error': np.float64(6.344871948854839e-05),\n",
       "   'Test RMSE': np.float64(0.0016017600141737228)}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425 202\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lars, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import LeaveOneOut, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import scipy.stats as stats\n",
    "\n",
    "# 如果尚未安装 xgboost 或 lightgbm，需要先执行:\n",
    "# pip install xgboost lightgbm\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============ 1. 读取数据 ============\n",
    "file_path = '../../matrix/otu_merged_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ============ 2. 构造 Drug Set 列 ============\n",
    "def map_drug_concentration(value):\n",
    "    if value == 0:\n",
    "        return \"none\"\n",
    "    elif value == 5 or value == 10:\n",
    "        return \"low\"\n",
    "    elif value == 50 or value == 100:\n",
    "        return \"high\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "drug_columns = [\n",
    "    'amoxicillin', \n",
    "    'oxytetracycline_dihydrate', \n",
    "    'sulfadiazine', \n",
    "    'trimethoprim', \n",
    "    'tylosin_tartrate', \n",
    "    'ciprofloxacin'\n",
    "]\n",
    "df['Drug Set'] = df[drug_columns].apply(\n",
    "    lambda row: '_'.join([map_drug_concentration(x) for x in row]), axis=1\n",
    ")\n",
    "\n",
    "# ============ 3. 过滤数据，保留指定菌科 ============\n",
    "bacterial_families = [\n",
    "    \"o__Bacillales;\", \"o__Lactobacillales;\", \"o__Enterobacteriales;\", \n",
    "    \"o__Burkholderiales;\", \"o__Actinomycetales;\", \"o__Aeromonadales;\", \n",
    "    \"o__Pseudomonadales;\"\n",
    "]\n",
    "\n",
    "df_filtered = df[\n",
    "    ['SampleID', 'Group', 'Isolation_source', 'Drug Set'] + bacterial_families\n",
    "]\n",
    "\n",
    "# ============ 4. 划分训练集和测试集 ============\n",
    "train_df = df_filtered[df_filtered['Group'].isin(['G1', 'G2', 'G3'])]\n",
    "test_df = df_filtered[df_filtered['Group'] == 'G4']\n",
    "\n",
    "# ============ 5. 对类别型变量做标签编码 ============\n",
    "label_encoders = {}\n",
    "for col in ['Isolation_source', 'Drug Set']:\n",
    "    le = LabelEncoder()\n",
    "    train_df[col] = le.fit_transform(train_df[col])\n",
    "    test_df[col] = le.transform(test_df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# ============ 6. 定义特征和目标 ============\n",
    "X_train = train_df[['Isolation_source', 'Drug Set']]\n",
    "y_train = train_df[bacterial_families]\n",
    "X_test = test_df[['Isolation_source', 'Drug Set']]\n",
    "y_test = test_df[bacterial_families]\n",
    "\n",
    "# ============ 7. 标准化特征 ============\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ============ 8. 定义模型和超参数网格 ============\n",
    "model_params = {\n",
    "    # Lars\n",
    "    'Least Angle Regression': (\n",
    "        Lars(),\n",
    "        {\n",
    "            'n_nonzero_coefs': [5, 10, 15, None]\n",
    "        }\n",
    "    ),\n",
    "    # Lasso\n",
    "    'Lasso': (\n",
    "        Lasso(max_iter=10000),\n",
    "        {\n",
    "            'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "        }\n",
    "    ),\n",
    "    # ElasticNet\n",
    "    'Elastic-Net': (\n",
    "        ElasticNet(max_iter=10000),\n",
    "        {\n",
    "            'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "            'l1_ratio': [0.1, 0.5, 0.9]\n",
    "        }\n",
    "    ),\n",
    "    # Random Forest\n",
    "    'Random Forest': (\n",
    "        RandomForestRegressor(random_state=42),\n",
    "        {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [None, 5, 10],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2]\n",
    "        }\n",
    "    ),\n",
    "    # XGBoost\n",
    "    'XGBoost': (\n",
    "        XGBRegressor(objective='reg:squarederror', random_state=42),\n",
    "        {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [3, 5],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'subsample': [0.8, 1.0]\n",
    "        }\n",
    "    ),\n",
    "    # LightGBM\n",
    "    'LightGBM': (\n",
    "        LGBMRegressor(random_state=42),\n",
    "        {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [3, 5],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'subsample': [0.8, 1.0]\n",
    "        }\n",
    "    )\n",
    "}\n",
    "\n",
    "# ============ 9. 进行训练和评价 ============\n",
    "results = []  # 用于存储所有结果，最后转换为 DataFrame\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "for family in bacterial_families:\n",
    "    y_train_family = y_train[family]\n",
    "    y_test_family = y_test[family]\n",
    "    \n",
    "    for model_name, (model, param_grid) in model_params.items():\n",
    "        # --------- 9.1 使用GridSearchCV + LOO寻找最优参数 ---------\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grid,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            cv=loo,  # LOO 交叉验证\n",
    "            n_jobs=-1  # 并行\n",
    "        )\n",
    "        grid_search.fit(X_train_scaled, y_train_family)\n",
    "        \n",
    "        # 最优模型和最优参数\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_params = grid_search.best_params_\n",
    "        \n",
    "        # --------- 9.2 使用最优模型，再次进行 LOO 来估计训练集性能 ---------\n",
    "        # （也可以直接从 grid_search.cv_results_ 中获取平均得分，但这里演示手动计算）\n",
    "        rmse_scores = []\n",
    "        for train_idx, valid_idx in loo.split(X_train_scaled):\n",
    "            X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[valid_idx]\n",
    "            y_tr, y_val = y_train_family.iloc[train_idx], y_train_family.iloc[valid_idx]\n",
    "            \n",
    "            best_model.fit(X_tr, y_tr)\n",
    "            y_val_pred = best_model.predict(X_val)\n",
    "            rmse_scores.append(\n",
    "                np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "            )\n",
    "        \n",
    "        rmse_mean = np.mean(rmse_scores)\n",
    "        rmse_sem = stats.sem(rmse_scores) if len(rmse_scores) > 1 else 0\n",
    "        \n",
    "        # --------- 9.3 用最优模型在测试集上进行预测并计算 RMSE ---------\n",
    "        best_model.fit(X_train_scaled, y_train_family)\n",
    "        y_pred_test = best_model.predict(X_test_scaled)\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test_family, y_pred_test))\n",
    "        \n",
    "        # --------- 9.4 将结果存入列表 ---------\n",
    "        results.append({\n",
    "            'Bacterial Family': family,\n",
    "            'Model': model_name,\n",
    "            'Best Params': best_params,\n",
    "            'Train RMSE Mean': rmse_mean,\n",
    "            'Train RMSE StdErr': rmse_sem,\n",
    "            'Test RMSE': test_rmse\n",
    "        })\n",
    "\n",
    "# ============ 10. 汇总输出结果为表格 ============\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"===== 训练与测试结果（部分展示） =====\")\n",
    "print(results_df.head(20))  # 可以自行调整显示数量\n",
    "\n",
    "# 如果希望完整展示，可用以下方式：\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "# print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
